<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ordinary_extract_table</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="table.css" />
</head>
<body>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 38%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr class="header">
<th>Folder</th>
<th>NameOfKernel</th>
<th>FunctionsRun</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceScanInitKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceScanKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void gemmk1_kernel</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void cutlass::Kernel2</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void pytorch_flash::flash_fwd_kernel</td>
<td>{cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void cublasLt::splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>ampere_s16816gemm_bf16_256x128_ldg8_stages_32x3_tn</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>ampere_bf16_s16816gemm_bf16_64x64_ldg8_f2f_stages_64x6_tn</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void gemv2T_kernel_val</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::_scatter_gather_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::mbtopk::fill</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::mbtopk::radixFindKthValues</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::mbtopk::computeBlockwiseWithinKCounts</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::mbtopk::computeBlockwiseKthCounts</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceScanByKeyInitKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceScanByKeyKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::mbtopk::gatherTopK</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::bitonicSortKVInPlace</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceRadixSortHistogramKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel</td>
<td>{at::native::, at::native::reduce_kernel, at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void at::native::_assert_async_cuda_kernel</td>
<td>{at::native::reduce_kernel}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void at::native::index_elementwise_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, cutlass::Kernel2}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>std::enable_if</td>
<td>{at::native::, at::native::vectorized_elementwise_kernel, cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="odd">
<td>medgemma</td>
<td>void pytorch_flash::flash_fwd_splitkv_kernel</td>
<td>{pytorch_flash::flash_fwd_kernel, pytorch_flash::flash_fwd_splitkv_kernel}</td>
</tr>
<tr class="even">
<td>medgemma</td>
<td>void pytorch_flash::flash_fwd_splitkv_combine_kernel</td>
<td>{cutlass::Kernel2, pytorch_flash::flash_fwd_splitkv_combine_kernel, pytorch_flash::flash_fwd_splitkv_kernel}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>void gemmSN_TN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>void at::native::elementwise_kernel</td>
<td>{at::native::elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>void gemmSN_NN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>std::enable_if</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>ampere_sgemm_32x128_tn</td>
<td>{at::native::elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>ampere_sgemm_128x32_tn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>ampere_sgemm_32x32_sliced1x4_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>void splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>ampere_sgemm_128x32_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>ampere_sgemm_32x128_nn</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>ampere_sgemm_32x32_sliced1x4_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>karpathy</td>
<td>ampere_sgemm_128x128_nt</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>karpathy</td>
<td>void at::native::reduce_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{at::native::}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void cutlass::Kernel</td>
<td>{at::native::, cutlass::Kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceReduceKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceReduceSingleTileKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceCompactInitKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceSelectSweepKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::index_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void gemmSN_TN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{at::native::elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void gemmSN_NN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>std::enable_if</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_tn</td>
<td>{at::native::elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_tn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_nn</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_128x128_nt</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemmSN_TN_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{at::native::elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemmSN_NN_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>std::enable_if</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_tn</td>
<td>{at::native::elementwise_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_tn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_nn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_nn</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_128x128_nt</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void gemmSN_TN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{at::native::elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void gemmSN_NN_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>std::enable_if</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_tn</td>
<td>{at::native::elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_tn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_128x32_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_32x128_nn</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_sgemm_32x32_sliced1x4_nn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_sgemm_128x128_nt</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, gemmSN_TN_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanInitKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemmk1_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemmk1_kernel</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void cutlass::Kernel2</td>
<td>{cutlass::Kernel2}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void pytorch_flash::flash_fwd_kernel</td>
<td>{cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void cublasLt::splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>ampere_s16816gemm_bf16_256x128_ldg8_stages_32x3_tn</td>
<td>{ampere_s16816gemm_bf16_256x128_ldg8_stages_32x3_tn, at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>ampere_bf16_s16816gemm_bf16_64x64_ldg8_f2f_stages_64x6_tn</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemv2T_kernel_val</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::_scatter_gather_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::mbtopk::fill</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::mbtopk::radixFindKthValues</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::mbtopk::computeBlockwiseWithinKCounts</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::mbtopk::computeBlockwiseKthCounts</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanByKeyInitKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanByKeyKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::mbtopk::gatherTopK</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::bitonicSortKVInPlace</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceRadixSortHistogramKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel</td>
<td>{at::native::}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::_assert_async_cuda_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::_assert_async_cuda_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::index_elementwise_kernel</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>std::enable_if</td>
<td>{at::native::, at::native::vectorized_elementwise_kernel, cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void pytorch_flash::flash_fwd_splitkv_kernel</td>
<td>{pytorch_flash::flash_fwd_splitkv_combine_kernel, pytorch_flash::flash_fwd_splitkv_kernel}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void gemmSN_TN_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{pytorch_flash::flash_fwd_kernel, pytorch_flash::flash_fwd_splitkv_combine_kernel, pytorch_flash::flash_fwd_splitkv_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanInitKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at_cuda_detail::cub::DeviceScanKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>pyblaz</td>
<td>void gemmk1_kernel</td>
<td>{at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>pyblaz</td>
<td>void cutlass::Kernel2</td>
<td>{at::native::vectorized_elementwise_kernel, cutlass::Kernel2, pytorch_flash::flash_fwd_kernel}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void at::native::</td>
<td>{at::native::}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>copy_info_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void geqr2_smem</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void larfb_set_v1_R_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void larft_vtv_32</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void larft_T_32</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>ampere_sgemm_32x32_sliced1x4_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>ampere_sgemm_128x32_tn</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>ampere_sgemm_32x128_nn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void larfb_restore_v1_R_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void at::native::triu_tril_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void orgqr_step1_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void larft_T_64_batch</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void batch_eye_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>ampere_sgemm_32x32_sliced1x4_nn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void lacpy_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>std::enable_if</td>
<td>{at::native::, at::native::reduce_kernel, geqr2_smem}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{at::native::reduce_kernel, at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="odd">
<td>linear-solver-cg</td>
<td>void dot_kernel</td>
<td>{dot_kernel}</td>
</tr>
<tr class="even">
<td>linear-solver-cg</td>
<td>void reduce_1Block_kernel</td>
<td>{at::native::elementwise_kernel, at::native::reduce_kernel, at::native::unrolled_elementwise_kernel, at::native::vectorized_elementwise_kernel, dot_kernel, reduce_1Block_kernel}</td>
</tr>
<tr class="odd">
<td>issue-157272</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-157272</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-157272</td>
<td>void at_cuda_detail::cub::DeviceReduceSingleTileKernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-157272</td>
<td>void at_cuda_detail::cub::DeviceCompactInitKernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-157272</td>
<td>void at_cuda_detail::cub::DeviceSelectSweepKernel</td>
<td>{at::native::elementwise_kernel, at::native::vectorized_elementwise_kernel}</td>
</tr>
<tr class="even">
<td>issue-156020</td>
<td>void vector_fft</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-156020</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{at::native::vectorized_elementwise_kernel, vector_fft}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>ampere_sgemm_64x64_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>fmha_cutlassF_f32_aligned_64x64_rf_sm80</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>ampere_sgemm_32x128_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>void at::native::</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>ampere_sgemm_128x64_tn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>ampere_sgemm_128x64_nn</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>ampere_sgemm_32x32_sliced1x4_nt</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>void cutlass::Kernel</td>
<td>{cutlass::Kernel}</td>
</tr>
<tr class="odd">
<td>issue-125674</td>
<td>void splitKreduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-125674</td>
<td>fmha_cutlassB_f32_aligned_64x64_k32_sm80</td>
<td>{at::native::, at::native::reduce_kernel, at::native::vectorized_elementwise_kernel, cutlass::Kernel}</td>
</tr>
<tr class="odd">
<td>issue-157237</td>
<td>void at::native::vectorized_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-157237</td>
<td>void at::native::unrolled_elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-157237</td>
<td>void at::native::reduce_kernel</td>
<td>{}</td>
</tr>
<tr class="even">
<td>issue-157237</td>
<td>void at::native::elementwise_kernel</td>
<td>{}</td>
</tr>
<tr class="odd">
<td>issue-157237</td>
<td>void cudnn::cnn::conv1D_NCHW_general</td>
<td>{cudnn::cnn::conv1D_NCHW_general}</td>
</tr>
</tbody>
</table>
</body>
</html>
